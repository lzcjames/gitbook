# Note 08/09/2022

## Google Cloud offerings
![](https://i.imgur.com/UDYMpN1.png)
Google Cloud offerings can be broadly categorized as compute, storage, big data, and  machine learning services for web, mobile, analytics, and backend solutions.The main  focus of this course is on big data and machine learning.
Google Cloud 提供的服务可以大致分为计算、存储、大数据和机器学习服务，用于 Web、移动、分析和后端解决方案。本课程的主要重点是大数据和机器学习。

![](https://i.imgur.com/xhCV92v.png)

There are 5 modules in today’s course, rounded off with a short summary and review  session.

下面是我们的日程安排: 在第一个模块中，您将了解 Google Cloud 上的大数据和机器学习。这包括 Google Cloud 的基础设施、大数据和机器学习产品。

Here’s our agenda:
- In the ﬁrst module, you’ll be introduced to big data and machine learning on Google Cloud. This includes Google Cloud’s infrastructure and big data and machine learning products.
在第一个模块中，您将了解 GoogleCloud 上的大数据和机器学习。这包括 Google Cloud 的基础设施、大数据和机器学习产品。
- In the second module of the course, you’ll explore data engineering for -streaming data. This includes how to build a streaming data pipeline, from ingestion with Pub/Sub, to processing with Dataﬂow, and ﬁnally, to visualization using Data Studio and Looker.
在本课程的第二个模块中，您将探索流数据的数据工程。这包括如何构建流式数据管道，从使用 Pub/Sub 进行摄取，到使用 Dataflow 进行处理，最后到使用 Data Studio 和 Looker 进行可视化。
- After that, you’ll explore big data with BigQuery, Google’s popular data warehouse tool, and BigQuery ML, the embedded ML functionality used for developing machine learning models directly in BigQuery.
之后，您将使用 BigQuery (谷歌流行的数据仓库工具)和 BigQuery ML (用于直接在 BigQuery 中开发机器学习模型的嵌入式机器学习功能)来探索大数据。
- From there, you’ll compare the four options provided by Google Cloud to build and deploy a machine learning model.
从那里开始，您将比较 Google Cloud 提供的构建和部署机器学习模型的四个选项。
- And in the ﬁnal module of the course, you’ll learn how to build a machine learning workﬂow from start to ﬁnish using Vertex AI, a uniﬁed platform that brings all the components of the machine learning ecosystem and workﬂow together.
在本课程的最后一个模块中，您将学习如何使用 Vertex AI 从头到尾构建一个机器学习工作流，这是一个统一的平台，将机器学习生态系统的所有组件和工作流结合在一起。


# 1. Big Data and Machine Learning on Google Cloud
Module 1
Google Cloud Big Data and Machine Learning Fundamentals


![](https://i.imgur.com/lOSc23R.png)

Welcome to the ﬁrst module of the Big Data and Machine Learning Fundamentals  course! This module lays the foundation to the next four modules,
- Data engineering
    - M.2: Data engineering for streaming data
    - M.3: Big data with BigQuery
- Machine learning
    - M.4: Machine learning options
    - M.5: The Machine Learning workﬂow with Vertex AI

## 1.1 Introduction
![](https://i.imgur.com/CH2PF6j.png)
You can think of the Google Cloud infrastructure in terms of three layers.

- At the base layer is networking and security, which lays the foundation to  support all of Google’s infrastructure and applications.
- On the next layer sit compute and storage. Google Cloud separates, or  decouples, as it’s technically called, compute and storage so they can scale  independently based on need.
- And on the top layer sit the big data and machine learning products, which  enable you to perform tasks to ingest, store, process, and deliver business  insights, data pipelines, and ML models.

And thanks to Google Cloud, these tasks can be accomplished without needing to  manage and scale the underlying infrastructure.

![](https://i.imgur.com/02R7bJs.png)
This course focuses on the middle layer, compute and storage, and the top layer, big  data and machine learning products.

Networking and security fall outside of the focus of this course, but if you’re  interested in learning more you can explore cloud.google.com/training for more  options.

## 1.2 Compute

![](https://i.imgur.com/e1WTEKy.png)

Let’s focus our attention on the middle layer of the Google Cloud infrastructure,
compute and storage. We’ll begin with compute.

![](https://i.imgur.com/a596Ho1.png)

Organizations with growing data needs often require lots of compute power to run big  data jobs. And as organizations design for the future, the need for compute power  only grows.

Google offers a range of computing services, which includes: Compute Engine,  Google Kubernetes Engine, App Engine, Cloud Functions, and Cloud Run.

![](https://i.imgur.com/6D1EbnA.png)

Let’s start with Compute Engine.

Compute Engine is an IaaS offering, or infrastructure as a service, which provides raw  compute, storage, and network capabilities organized virtually into resources that are  similar to physical data centers. It provides maximum ﬂexibility for those who prefer  to manage server instances themselves.

![](https://i.imgur.com/hWuJvrW.png)

The second is Google Kubernetes Engine, or GKE.

GKE runs containerized applications in a cloud environment, as opposed to on an  individual virtual machine, like Compute Engine. A container represents code  packaged up with all its dependencies.

![](https://i.imgur.com/OMMzB5h.png)

The third computing service offered by Google is App Engine, a fully managed PaaS  offering, or platform as a service. PaaS offerings bind code to libraries that provide  access to the infrastructure application needs. This allows more resources to be  focused on application logic.

![](https://i.imgur.com/fhs586D.png)

The fourth is Cloud Functions, which executes code in response to events, like when a  new ﬁle is uploaded to Cloud Storage. It’s a completely serverless execution  environment, often referred to as functions as a service.

![](https://i.imgur.com/Ek2ZHs7.png)

And ﬁnally there is Cloud Run, a fully managed compute platform that enables you to  run request or event-driven stateless workloads without having to worry about  servers. It abstracts away all infrastructure management so you can focus on writing  code. It automatically scales up and down from zero, so you never have to worry  about scale configuration.

Cloud Run charges you only for the resources you use so you never pay for over  provisioned resources.

![](https://i.imgur.com/Vq9Hlhc.png)

Google Photos offers a feature called automatic video stabilization. This takes an  unstable video, like one captured while riding on the back of a motorbike, and  stabilizes it to minimize movement.

Let’s look at an example of a technology that requires a lot of compute power.

![](https://i.imgur.com/lLwtAtQ.jpg)

![](https://i.imgur.com/SaSwyCF.png)

For this feature to work as intended, you need the proper data. This includes the video  itself, which is really a large collection of individual images, along with time series  data on the camera’s position and orientation from the onboard gyroscope, and  motion from the camera lens.

A short video can require over a billion data points to feed the ML model to create a  stabilized version. As of 2020, roughly 28 billion photos and videos were uploaded to  Google Photos every week, with more than four trillion photos in total stored in the  service.

![](https://i.imgur.com/nyHptx4.png)

To ensure that this feature works as intended, and accurately, the Google Photos team  needed to develop, train, and serve a high-performing machine learning model on  millions of videos. That’s a large training dataset!

![](https://i.imgur.com/lhJ4JS6.png)

Just as the hardware on a standard personal computer might not be powerful enough  to process a big data job for an organization, the hardware on a smartphone is not  powerful enough to train sophisticated ML models.

That’s why Google trains production machine learning models on a vast network of  data centers, only to then deploy smaller, trained versions of the models to the  smartphone and personal computer hardware.

![](https://i.imgur.com/1x6aR3G.png)

But where does all that processing power come from?

According to Stanford University’s 2019 AI index report, before 2012, artiﬁcial  intelligence results tracked closely with Moore’s Law, with the required computing  power used in the largest AI training runs doubling every two years. The report states  that, since 2012, the required computing power has been doubling approximately  every three and a half months.

![](https://i.imgur.com/WBelgd6.png)

This means that hardware manufacturers have run up against limitations, and CPUs,  which are central processing units, and GPUs, which are graphics processing units,  can no longer scale to adequately reach the rapid demand for ML.

![](https://i.imgur.com/Svq38Ml.png)

To help overcome this challenge, in 2016 Google introduced the Tensor Processing  Unit, or TPU. TPUs are Google’s custom-developed application-speciﬁc integrated  circuits (ASICs) used to accelerate machine learning workloads.

TPUs act as domain-speciﬁc hardware, as opposed to general-purpose hardware  with CPUs and GPUs. This allows for higher eﬃciency by tailoring architecture to meet  the computation needs in a domain, such as the matrix multiplication in machine learning.

![](https://i.imgur.com/vzbZ3Cc.png)

With TPUs, the computing speed increases more than 200 times.

This means that instead of waiting 26 hours for results with a single state-of-art GPU,  you’ll only need to wait for 7.9 minutes for a full Cloud TPU v.2 pod to deliver the same  results.

Cloud TPUs have been integrated across Google products, and this state-of-the-art  hardware and supercomputing technology is available with Google Cloud products  and services.

## 1.3 Storage

![](https://i.imgur.com/wmzp0my.png)
Now that we’ve explored compute and why it’s needed for big data and ML jobs, let’s  now examine storage.

![](https://i.imgur.com/sf0OfR9.png)

For proper scaling capabilities, compute and storage are decoupled. This is one of the  major differences between cloud computing and desktop computing.

With cloud computing, processing limitations aren’t attached to storage disks.

![](https://i.imgur.com/wRZCPGJ.png)

Most applications require a database and storage solution of some kind.

With Compute Engine, for example, which was mentioned previously, you can install  and run a database on a virtual machine, just as you would do in a data center.

![](https://i.imgur.com/6UIY94M.png)


These include:
- Cloud Storage
- Cloud Bigtable
- Cloud SQL
- Cloud Spanner, and
- Firestore
- BigQuery

The goal of these products is to reduce the time and effort needed to store data. This  means creating an elastic storage bucket directly in a web interface or through a  command line for example on Google Cloud Storage.

![](https://i.imgur.com/HW0pn3g.png)


Google Cloud offers relational and non-relational databases, and worldwide object  storage.

Choosing the right option to store and process data often depends on the data type  that needs to be stored and the business need.

![](https://i.imgur.com/BMreQXJ.png)

Let’s start with unstructured versus structured data.

Unstructured data is information stored in a non-tabular form such as documents,  images, and audio ﬁles. Unstructured data is usually best suited to Cloud Storage.

![](https://i.imgur.com/PykTLRk.png)

Cloud Storage has four primary storage classes.
- The ﬁrst is Standard Storage. Standard Storage is considered best for  frequently accessed, or “hot,” data. It’s also great for data that is stored for  only brief periods of time.
- The second storage class is Nearline Storage. This is best for storing  infrequently accessed data, like reading or modifying data once per month or  less, on average. Examples include data backups, long-tail multimedia content,  or data archiving.
- The third storage class is Coldline Storage. This is also a low-cost option for  storing infrequently accessed data. However, as compared to Nearline  Storage, Coldline Storage is meant for reading or modifying data, at most,  once every 90 days.
- The fourth storage class is Archive Storage. This is the lowest-cost option,  used ideally for data archiving, online backup, and disaster recovery. It’s the  best choice for data that you plan to access less than once a year, because it  has higher costs for data access and operations and a 365-day minimum  storage duration.

![](https://i.imgur.com/d0IcMZ0.png)
Alternatively, there is structured data, which represents information stored in tables,  rows, and columns.

![](https://i.imgur.com/yUdaQ5A.png)

Structured data comes in two types: transactional workloads and analytical
workloads.

- Transactional workloads stem from Online Transaction Processing systems,  which are used when fast data inserts and updates are required to build
row-based records. This is usually to maintain a system snapshot. They  require relatively standardized queries that impact only a few records.
    - So, if your data is transactional and you need to access it using SQL,  then Cloud SQL and Cloud Spanner are two options.
        - Cloud SQL works best for local to regional scalability,
        - while Cloud Spanner, it best to scale a database globally.
    - If the transactional data will be accessed without SQL,
        - Firestore might be the best option. Firestore is a transactional  No-SQL, document-oriented database.

- Then there are analytical workloads, which stem from Online Analytical  Processing systems, which are used when entire datasets need to be read.  They often require complex queries, for example, aggregations.
    - If you have analytical workloads that require SQL commands, BigQuery  is likely the best option. BigQuery, Google’s data warehouse solution,  lets you analyze petabyte-scale datasets.
    - Alternatively, Cloud Bigtable provides a scalable NoSQL solution for  analytical workloads. It’s best for real-time, high-throughput  applications that require only millisecond latency.

## 1.4 The history of big  data and ML products
![](https://i.imgur.com/YRqtXp0.png)

![](https://i.imgur.com/WYSM6hU.png)

The ﬁnal layer of the Google Cloud infrastructure that is left to explore is big data and  machine learning products.

We’ll examine the evolution of data processing frameworks through the lens of  product development. Understanding the chronology of products can help address  typical big data and machine learning challenges.

![](https://i.imgur.com/Oc3KKsz.png)
Historically speaking, Google experienced challenges related to big data quite  early–mostly with large datasets, fast-changing data, and varied data. This was the  result of needing to index the World Wide Web.

And as the internet grew, Google needed to invent new data processing methods.

- So, in 2002, Google released the Google File System, or GFS. GFS was  designed to handle data sharing and petabyte storage at scale. It served as  the foundation for Cloud Storage and also what would become the managed  storage functionality in BigQuery.

- A challenge that Google was facing around this time was how to index the  exploding volume of content on the web. To solve this, in 2004 Google wrote a  report that introduced MapReduce. MapReduce was a new style of data  processing designed to manage large-scale data processing across big  clusters of commodity servers.

- As Google continued to grow, new challenges arose, speciﬁcally with recording  and retrieving millions of streaming user actions with high throughput. The  solution was the release in 2005 of Cloud Bigtable, a high-performance NoSQL  database service for large analytical and operational workloads.

With MapReduce available, some developers were restricted by the need to write code  to manage their infrastructure, which prevented them from focusing on applicationlogic.

As a result, from 2008 to 2010, Google started to move away from MapReduce as the  solution to process and query large datasets.

- So, in 2008, Dremel was introduced. Dremel took a new approach to big-data  processing by breaking the data into smaller chunks called shards, and then  compressing them.
- Dremel then uses a query optimizer to share tasks between the many shards  of data and the Google data centers, which processed queries and delivered results.  The big innovation was that Dremel autoscaled to meet query demands.
Dremel became the query engine behind BigQuery.

Google continued innovating to solve big data and machine learning challenges.  Some of the technology solutions released include:

- Colossus, in 2010, which is a cluster-level ﬁle system and successor to the  Google File System.
- BigQuery, in 2010 as well, which is a fully-managed, serverless data  warehouse that enables scalable analysis over petabytes of data. It is a  - - Platform as a Service (PaaS) that supports querying using ANSI SQL. It also  has built-in machine learning capabilities. BigQuery was announced in May  2010 and made generally available in November 2011.
- Spanner, in 2012, which is a globally available and scalable relational  database.
- Pub/Sub, in 2015, which is a service used for streaming analytics and data  integration pipelines to ingest and distribute data.

And TensorFlow, also in 2015, which is a free and open source software library for  machine learning and artiﬁcial intelligence.

- 2018 brought the release of the Tensor Processing Unit, or TPU, which you’ll  recall from earlier, and
- AutoML, as a suite of machine learning products.
- The list goes on till Vertex AI, a uniﬁed ML platform released in 2021.


![](https://i.imgur.com/u1BsC0B.png)

And it’s thanks to these technologies that the big data and machine learning product  line is now robust.

This includes:

- Cloud Storage
- Dataproc
- Cloud Bigtable
- BigQuery
- Dataﬂow
- Firestore
- Pub/Sub
- Looker
- Cloud Spanner
- AutoML, and
- Vertex AI, the uniﬁed platform

These products and services are made available through Google Cloud, and you’ll get  hands-on practice with some of them as part of this course.

## 1.5 Big data and ML  product categories
![](https://i.imgur.com/gs15WHr.png)

![](https://i.imgur.com/ulXkloq.png)

As we explored previously, Google offers a range of big data and machine learning  products. So, how do you know which is best for your business needs?

Let’s look closer at the list of products, which can be divided into four general  categories along the data-to-AI workﬂow: ingestion and process, storage, analytics,  and machine learning.

Understanding these product categories can help narrow down your choice.

![](https://i.imgur.com/hd3ZT3v.png)

The ﬁrst category is ingestion and process, which include products that are used to  digest both real-time and batch data. The list includes:

- Pub/Sub
- Dataﬂow
- Dataproc
- Cloud Data Fusion

You’ll explore how Dataﬂow and Pub/Sub can ingest streaming data later in this  course.

![](https://i.imgur.com/nOsiHQN.png)

The second product category is data storage, and you’ll recall from earlier that there  are ﬁve storage products:

- Cloud Storage
- Cloud SQL
- Cloud Spanner
- Cloud Bigtable, and
- Firestore

Cloud SQL and Cloud Spanner are relational databases, while Bigtable and Firestore  are NoSQL databases.

![](https://i.imgur.com/1IqEuwW.png)

The third product category is analytics. The major analytics tool is BigQuery. BigQuery  is a fully managed data warehouse that can be used to analyze data through SQL  commands.

In addition to BigQuery, you can analyze data and visualize results using:
- Google Data Studio, and
- Looker

You’ll explore BigQuery, Looker, and Data Studio in this course.

![](https://i.imgur.com/MOMQ6RS.png)

And the ﬁnal product category is machine learning, or ML. ML products include both  the ML development platform and the AI solutions:

The primary product of the ML development platform is Vertex AI, which includes:
- AutoML,
- Vertex AI Workbench, and
- TensorFlow

You’ll explore Vertex AI and AutoML in this course.

![](https://i.imgur.com/2STMkLt.png)

AI solutions are built on the ML development platform and include state-of-the-art  products to meet both horizontal and vertical market needs. These include:
- Document AI
- Contact Center AI
- Retail Product Discovery, and
- Healthcare Data Engine

These products unlock insights that only large amounts of data can provide. We’ll  explore the machine learning options and workﬂow together with these products in  greater detail later.

## 1.6 Customer example:  Gojek

![](https://i.imgur.com/3nX6HoX.png)

With many big data and machine learning products options available, it can be helpful  to see an example of how an organization has leveraged Google Cloud to meet their  goals.

![](https://i.imgur.com/d9XpbSX.png)

In this section, you’ll learn about a company called Gojek and how they were able to  ﬁnd success through Google Cloud’s data engineering and machine learning  offerings.

## 1.7 Lab: Exploring a BigQuery  public dataset

![](https://i.imgur.com/bCJsbut.png)

In this lab, you’ll use BigQuery to explore a public dataset.

You’ll practice:
- Querying a public data set
- Creating a custom table
- Loading data into a table, and
- Querying a table

## 1.8 Summary
![](https://i.imgur.com/A8k4hpy.png)

### 1.8.1 
![](https://i.imgur.com/oQ6F0Ef.png)

This brings us to the end of the ﬁrst module of the Big Data and Machine Learning  Fundamentals course. Before we move forward, let’s review what we’ve covered so  far.

You began by exploring the Google Cloud infrastructure through three different layers.

![](https://i.imgur.com/44OtiVI.png)

At the base layer is **networking and security**, which makes up the foundation to  support all of Google’s infrastructure and applications.

On the next layer sit **compute** and **storage**. Google Cloud decouples compute and  storage so they can scale independently based on need.

And on the top layer sit the **big data and machine learning products**.

### 1.8.2 
![](https://i.imgur.com/wNX9qrt.png)

In the next section, you learned about the history of big data and ML technologies,

![](https://i.imgur.com/bc9WVdq.png)

Google has been working with data and artiﬁcial intelligence since its early days as a  company, starting from GFS (Google File system), to BigQuery (Google’s fully-managed  data warehouse), and to TensorFlow (open source ML library), TPU (Tensor Processing  Unit), and recently Vertex AI (a uniﬁed ML platform).

![](https://i.imgur.com/QzFTNfu.png)

And ﬁnally explored the four major product categories: **Ingestion and process**,  **storage**, **analytics**, and **machine learning**.

# 2. Data Engineering for  Streaming Data

![](https://i.imgur.com/VOtdKd3.png)

## 2.1 Introduction

![](https://i.imgur.com/H9C8ooO.png)
![](https://i.imgur.com/fSnokRW.png)

In previous module of this course, you learned Google Cloud architecture such as  storage and compute. You were also introduced the data and machine learning  products on Google Cloud to support the data-to-AI lifecycle. 

In the next two modules,  you’ll explore Google products and technologies in data engineering. In this module,  you’ll explore data engineering for streaming data with the goal of building a real-time  data solution with Google Cloud products and services.

![](https://i.imgur.com/q4hh7vU.png)

Coming up in this module, you’ll:

- Examine some of the big data challenges faced by today’s data engineers  when setting up and managing pipelines.
- Learn about message-oriented architecture. This includes ways to capture  streaming messages globally, reliably, and at scale so they can be fed into a  pipeline.
- See how to design streaming pipelines with Apache Beam, and then
implement them with Dataﬂow.
- Explore how to visualize data insights on a dashboard with Looker and Data  Studio.
- Get hands-on practice building an end-to-end data pipeline that handles  real-time data ingestion with Pub/Sub, processing with Dataﬂow, and  visualization with Data Studio.

![](https://i.imgur.com/A4ROGi9.png)

Before we get too far, let’s take a moment to explain what streaming data is, how it  differs from batch processing, and why it’s important.

- Batch processing is when the processing and analysis happens on a set of  stored data. An example is payroll and billing systems that have to be  processed on either a weekly or monthly basis.
- Streaming data is a ﬂow of data records generated by various data sources.  The processing of streaming data happens as the data ﬂows through a  system. This results in the analysis and reporting of events as they happen.  An example would be fraud detection or intrusion detection.
Streaming data processing means that the data is analyzed in near-real time  and that actions will be taken on the data as quickly as possible.

Modern data processing has progressed from legacy batch processing of data toward  working with real-time data streams. An example of this is streaming music and  movies. No longer is it necessary to download an entire movie or album to a local  device. Data streams are a key part in the world of big data.

## 2.2 Big data challenges
![](https://i.imgur.com/04u7iOe.png)

![](https://i.imgur.com/3necg8I.png)

Building scalable and reliable pipelines is a core responsibility of data engineers.  However, in modern organizations, data engineers and data scientists are facing four  major challenges.
These are collectively known as the 4 Vs.  They are variety, volume, velocity, and veracity.

![Uploading file..._wr7hyjuy3]()

![](https://i.imgur.com/p8tWEU4.png)

First, data could come in from a variety of different sources and in various formats.  Imagine hundreds of thousands of sensors for self-driving cars on roads around the  world. The data is returned in various formats such as number, image, or even audio.

![](https://i.imgur.com/HTr973m.png)

Now consider point-of-sale data from a thousand different stores. How do we alert  our downstream systems of new transactions in an organized way with no  duplicates?

![](https://i.imgur.com/33DwXtB.png)

Next, let’s increase the magnitude of the challenge to handle not only an arbitrary  variety of input sources, but a volume of data that varies from gigabytes to petabytes.

You’ll need to know whether your pipeline code and infrastructure can scale with  those changes or whether it will grind to a halt or even crash.

![](https://i.imgur.com/qnnegZV.png)

The third challenge concerns velocity. Data often needs to be processed in near-real  time, as soon as it reaches the system.

![](https://i.imgur.com/IUQfIRz.png)

You’ll probably also need a way to handle data that arrives late, has bad data in the  message, or needs to be transformed mid-ﬂight before it is streamed into a data  warehouse.

![](https://i.imgur.com/mcNuOBD.png)

And the fourth major challenge is veracity, which refers to the data quality. Because  big data involves a multitude of data dimensions resulting from different data types  and sources, there’s a possibility that gathered data will come with some  inconsistencies and uncertainties.

Challenges like these are common considerations for pipeline developers. By the end  of this module, the goal is for you to better understand the tools available to help  successfully build a streaming data pipeline and avoid these challenges.

## 2.3 Message-oriented  architecture

![](https://i.imgur.com/Nx95qUE.png)

![](https://i.imgur.com/x3KEs0X.png)

One of the early stages in a data pipeline is data ingestion, which is where large  amounts of streaming data are received.

![](https://i.imgur.com/TA9hViW.png)

Data, however, may not always come from a single, structured database. Instead, the  data might stream from a thousand, or even a million, different events that are all  happening asynchronously.

A common example of this is data from IoT, or Internet of Things, applications.

These can include sensors on taxis that send out location data every 30 seconds or  temperature sensors around a data center to help optimize heating and cooling.

![](https://i.imgur.com/tI3X1Jq.png)

These IoT devices present new challenges to data ingestion, which can be  summarized in four points:

1. The ﬁrst is that **data can be streamed from many different methods and  devices**, many of which might not talk to each other and might be sending bad  or delayed data.
2. The second is that **it can be hard to distribute event messages** to the right  subscribers. Event messages are notiﬁcations. A method is needed to collect  the streaming messages that come from IoT sensors and broadcast them to  the subscribers as needed.
3. The third is that **data can arrive quickly and at high volumes**. Services must be  able to support this.
4. And the fourth challenge is **ensuring services are reliable, secure, and  perform as expected.**

![](https://i.imgur.com/FEz9BOe.png)
These IoT devices present new challenges to data ingestion, which can be  summarized in four points:

1. The ﬁrst is that data can be streamed from many different methods and  devices, many of which might not talk to each other and might be sending bad  or delayed data.
2. The second is that it can be hard to distribute event messages to the right  subscribers. Event messages are notiﬁcations. A method is needed to collect  the streaming messages that come from IoT sensors and broadcast them to  the subscribers as needed.
3. The third is that data can arrive quickly and at high volumes. Services must be  able to support this.
4. And the fourth challenge is ensuring services are reliable, secure, and  perform as expected.

![](https://i.imgur.com/ABJkc5m.png)

Google Cloud has a tool to handle distributed message-oriented architectures at  scale, and that is Pub/Sub. The name is short for Publisher/Subscriber, or publish  messages to subscribers.

Pub/Sub is a distributed messaging service that can receive messages from a variety  of device streams such as gaming events, IoT devices, and application streams.

![](https://i.imgur.com/OkPxjsQ.png)
Pub/Sub ensures at-least-once delivery of received messages to subscribing  applications, with no provisioning required.

Pub/Sub’s APIs are open, the service is global by default, and it offers end-to-end  encryption.

![](https://i.imgur.com/SKdqVdm.png)

Let’s explore the end-to-end architecture using Pub/Sub.

- Upstream source data comes in from devices all over the globe and is  ingested into Pub/Sub, which is the ﬁrst point of contact within the system.  Pub/Sub reads, stores, and broadcasts to any subscribers of this data topic  that new messages are available.
- As a subscriber of Pub/Sub, Dataﬂow can ingest and transform those  messages in an elastic streaming pipeline and output the results into an  analytics data warehouse like BigQuery.
- Finally, you can connect a data visualization tool, like Looker or Data Studio, to  visualize and monitor the results of a pipeline, or an AI or ML tool such as  Vertex AI to explore the data to uncover business insights or help with  predictions.

![](https://i.imgur.com/txV46H9.png)

A central element of Pub/Sub is the **topic**.

You can think of a topic like a radio antenna. Whether your radio is playing music or  it’s turned off, the antenna itself is always there. If music is being broadcast on a  frequency that nobody’s listening to, the stream of music still exists. Similarly, a  publisher can send data to a topic that has no subscriber to receive it. Or a subscriber  can be waiting for data from a topic that isn’t getting data sent to it, like listening to  static from a bad radio frequency. Or you could have a fully operational pipeline where  the publisher is sending data to a topic that an application is subscribed to.

That means there can be zero, one, or more publishers, and zero, one or more  subscribers related to a topic. And they’re completely decoupled, so they’re free to  break without affecting their counterparts.

![](https://i.imgur.com/XRhBp7y.png)

It’s helpful to describe this using an example.

Say you’ve got a human resources topic. A new employee joins your company, and  several applications across the company need to be updated. Adding a new employee  can be an event that generates a notiﬁcation to the other applications that are  subscribed to the topic, and they’ll receive the message about the new employee  starting.

Now, let’s assume that there are two different types of employees: a full-time  employee and a contractor. Both sources of employee data could have no knowledge  of the other but still publish their events saying “this employee joined” into the  Pub/Sub HR topic.

After Pub/Sub receives the message, downstream applications like the directory  service, facilities system, account provisioning, and badge activation systems can all  listen and process their own next steps independent of one another.

![](https://i.imgur.com/K5ATLfs.png)

Pub/Sub is a good solution to buffer changes for lightly coupled architectures, like  this one, that have many different publishers and subscribers.

Pub/Sub supports many different inputs and outputs, and you can even publish a  Pub/Sub event from one topic to another.

The next task is to get these messages reliably into our data warehouse, and we’ll  need a pipeline that can match Pub/Sub’s scale and elasticity to do it.

## 2.4 Designing streaming pipelines with Apache Beam
![](https://i.imgur.com/wQE6a21.png)

![](https://i.imgur.com/xliWVhK.png)

After messages have been captured from the streaming input sources, you need a  way to pipe that data into a data warehouse for analysis. This is where Dataﬂow  comes in.

Dataﬂow creates a pipeline to process both streaming data and batch data. “Process”  in this case refers to the steps to extract, transform, and load data, or ETL.

![](https://i.imgur.com/I5EJbwK.png)

When building a data pipeline, data engineers often encounter challenges related to  coding the pipeline design and implementing and serving the pipeline at scale.

![](https://i.imgur.com/ECp03WB.png)
During the pipeline design phase, there are a few questions to consider:
- Will the pipeline code be compatible with both batch and streaming data, or  will it need to be refactored?
- Will the pipeline code software development kit, or SDK, being used have all  the transformations, mid-ﬂight aggregations and windowing? and
- Be able to handle late data?
- Are there existing templates or solutions that should be referenced?

![](https://i.imgur.com/2ppbGLS.png)
A popular solution for pipeline design is Apache Beam. It’s an open source, uniﬁed  programming model to deﬁne and execute data processing pipelines, including ETL,  batch, and stream processing.

![](https://i.imgur.com/U7BMpTk.png)
- Apache Beam is **uniﬁed**, which means it uses a single programming model for  both batch and streaming data.
- It’s **portable**, which means it can work on multiple execution environments, like  Dataﬂow and Apache Spark, among others.
- And it’s **extensible**, which means it allows you to write and share your own  connectors and transformation libraries.
- Apache Beam provides **pipeline templates**, so you don’t need to build a  pipeline from nothing. And it can write pipelines in Java, Python, or Go.
- The Apache Beam software development kit, or SDK, is a collection of  software development tools in one installable package. It provides a variety of  libraries for transformations and data connectors to sources and sinks.
- Apache Beam creates a **model representation** from your code that is portable  across many runners. Runners pass off your model for execution on a variety  of different possible engines, with Dataﬂow being a popular choice.

## 2.5 Implementing  streaming pipelines  on Cloud Dataflow

![](https://i.imgur.com/f7IxSyJ.png)

![](https://i.imgur.com/wwQj8Zb.png)

As you just saw, Apache Beam can be used to create data processing pipelines. The  next step is to identify an execution engine to implement those pipelines.

![](https://i.imgur.com/4hbEFd2.png)

When choosing an execution engine for your pipeline code, it might be helpful to  consider the following questions:

1. How much maintenance overhead is involved?
2. Is the infrastructure reliable?
3. How is the pipeline scaling handled?
4. How can the pipeline be monitored?
5. Is the pipeline locked in to a speciﬁc service provider?

![](https://i.imgur.com/t1lNXpN.png)
This brings us to **Dataﬂow**. Dataﬂow is a fully managed service for executing Apache  Beam pipelines within the Google Cloud ecosystem.

Dataﬂow handles much of the complexity relating to infrastructure setup and  maintenance and is built on Google’s infrastructure.

This allows for reliable auto scaling to meet data pipeline demands.

![](https://i.imgur.com/PdPC871.png)

Dataﬂow is NoOps (No Operations) and serverless.
- A NoOps environment is one that doesn't require management from an  operations team, because maintenance, monitoring, and scaling are  automated.
- Serverless computing is a cloud computing execution model. This is when  -Google Cloud, for example, manages infrastructure tasks on behalf of the  users. This includes tasks like resource provisioning, performance tuning, and  ensuring pipeline reliability.

Using a serverless and NoOps solution like Dataﬂow means that you can spend more  time analyzing the insights from your datasets and less time provisioning resources  to ensure that your pipeline will successfully complete its next cycles. It’s designed to  be low maintenance.

![](https://i.imgur.com/otbSEX1.png)

Let’s explore the tasks Dataﬂow performs when a job is received.

- It starts by optimizing a pipeline model's execution graph to remove any  ineﬃciencies.
- Next, it schedules out distributed work to new workers and scales as needed.
- After that, it auto-heals any worker faults.
- From there, it automatically rebalances efforts to most eﬃciently use its  workers.
- And ﬁnally, it outputs data to produce a result. BigQuery is one of many  options that data can be outputted to. You’ll get some more practice using  BigQuery later in this course.

By design, you don't need to monitor all of the compute and storage resources that  Dataﬂow manages, to ﬁt the demand of a streaming data pipeline.

![](https://i.imgur.com/t11dErp.png)

Even experienced Java or Python developers will beneﬁt from using Dataﬂow  templates, which cover common use cases across Google Cloud products.

The list of templates is continuously growing. They can be broken down into three  categories: **streaming templates**, **batch templates**, and **utility templates**.

- **Streaming templates** are for processing continuous, or real-time, data.  For example:
    - Pub/Sub to BigQuery
    - Pub/Sub to Cloud Storage
    - Datastream to BigQuery
    - Pub/Sub to MongoDB

- **Batch templates** are for processing bulk data, or batch load data.  For example:
    - BigQuery to Cloud Storage
    - Bigtable to Cloud Storage
    - Cloud Storage to BigQuery
    - Cloud Spanner to Cloud Storage

Finally, **utility templates** address activities related to bulk compression, deletion, and  conversion.

For a complete list of templates, please refer to the reading list.

## 2.6 Visualization with Looker
![](https://i.imgur.com/zAapfNR.png)

![](https://i.imgur.com/voq2UGW.png)
Telling a good story with data through a dashboard can be critical to the success of a  data pipeline, because data that is diﬃcult to interpret or draw insights from might be  useless. After data is in BigQuery, a lot of skill and effort can still be required to  uncover insights.

To help create an environment where stakeholders can easily interact with and  visualize data, Google Cloud offers two solutions: **Looker** and **Google Data Studio**.

![](https://i.imgur.com/CYcRBKk.png)

Let’s explore both of them, starting with **Looker**.

Looker supports BigQuery, as well as more than 60 different SQL databases.

![](https://i.imgur.com/H2pkPL6.png)

It allows developers to deﬁne a semantic modeling layer on top of databases using **Looker Modeling Language**, or **LookML**.

**LookML** deﬁnes logic and permissions independent from a speciﬁc database or a  SQL language, which frees a data engineer from interacting with individual databases  to focus more on business logic across an organization.

![](https://i.imgur.com/e6RA8vg.png)

The Looker platform is 100% web-based, which makes it easy to integrate into  existing workﬂows and share with multiple teams at an organization.

![](https://i.imgur.com/o4UjIxL.png)

There is also a Looker API, which can be used to embed Looker reports in other  applications.

![](https://i.imgur.com/v80hp37.png)

Let’s explore some of Looker’s features, starting with dashboards.

Dashboards, like the Business Pulse dashboard, for example, can visualize data in a  way that makes insights easy to understand.

For a sales organization, it shows ﬁgures that many might want to see at the start of  the week, like the number of new users acquired, monthly sales trends, and even the  number of year-to-date orders. Information like this can help align teams, identify  customer frustrations, and maybe even uncover lost revenue.

Based on the metrics that are important to your business, you can create Looker  dashboards that provide straightforward presentations to help you and your  colleagues quickly see a high-level business status.

![](https://i.imgur.com/3odilUV.png)

Looker has multiple data visualization options, including area charts, line charts,  Sankey diagrams, funnels, and liquid ﬁll gauges.

![](https://i.imgur.com/DrhBqR5.png)

To share a dashboard with your team, you schedule delivery through storage services  like Google Drive, Slack, and Dropbox.

![](https://i.imgur.com/hh0ytuh.png)

Let’s explore another Looker dashboard, this time one that monitors key metrics  related to New York City taxis over a period of time.

This dashboard displays:
- Total revenue
- Total numbers of passengers, and
- Total number of rides

Looker displays this information through a timeseries to help monitor metrics over  time. Looker also lets you plot data on a map to see ride distribution, busy areas, and  peak hours.

The purpose of these features is to help you draw insights to make business  decisions.

For more training on Looker, please refer to cloud.google.com/training.

## 2.7 Visualization  with Data Studio

![](https://i.imgur.com/LC5bckT.png)

![](https://i.imgur.com/BLC9Uc8.png)

Another popular data visualization tool offered by Google is **Data Studio**.

Data Studio is **integrated into BigQuery**, which makes data visualization possible with  just a few clicks. This means that leveraging Data Studio doesn’t require support from  an administrator to establish a data connection, which is a requirement with Looker.

![](https://i.imgur.com/C9FQRUX.png)

Data Studio dashboards are widely used across many Google products and  applications.

For example, Data Studio is integrated into **Google Analytics** to help visualize,  in this case, a summary of a marketing website.
This dashboard visualizes the total number of visitors through a map,  compares month-over-month trends, and even displays visitor distribution by age.
Another Data Studio integration is the Google Cloud **billing dashboard**. You  might be familiar with this from your account. Maybe you’ve already used it to  monitor spending, for example.

![](https://i.imgur.com/Jjd7zhx.png)

You’ll soon have hands-on practice with Data Studio, but in preparation for the lab,  let’s explore the three steps needed to create a Data Studio dashboard.

First, **choose a template**. You can start with either a pre-built template or a blank  report.

Second, **link the dashboard to a data source**. This might come from BigQuery, a local  ﬁle, or a Google application like Google Sheets or Google Analytics–or a combination  of any of these sources.

And third, **explore your dashboard!**

## 2.8 Lab: Creating a Streaming Data  Pipeline for a Real-time  Dashboard with Dataflow

![](https://i.imgur.com/kOSirV6.png)

![](https://i.imgur.com/GiwFXfp.png)

Now it’s time for hands-on practice with some of the tools you learned about in this  module of the course.

In the lab that follows, you’ll build a streaming data pipeline to monitor sensor data,  and then visualize the dataset through a dashboard.

![](https://i.imgur.com/qLUVt9X.png)

You’ll practice:

- Creating a Dataﬂow job from a pre-existing template and subscribing to a  Pub/Sub topic
- Streaming and monitoring a Dataﬂow pipeline into BigQuery
- Analyzing results with SQL, and
- Visualizing key metrics in Data Studio.

Please note that, though you will use some SQL commands in this lab, the lab doesn’t  actually require strong SQL knowledge. We’ll explore BigQuery in more detail later in  this course.

## 2.9 Summary

![](https://i.imgur.com/PLDcdw5.png)

Well done completing the lab on building a data pipeline for streaming data!

Before you move on in the course, let's do a quick recap.

![](https://i.imgur.com/86LHKra.png)

In this module, you explored the streaming data workﬂow, from ingestion to  visualization.

![](https://i.imgur.com/nr52sQb.png)

You started by learning about the four common big data challenges, or the 4Vs:
- Volume (data size)
- Variety (data format)
- Velocity (data speed), and
- Veracity (data accuracy).

These challenges can be especially common when dealing with streaming data.

![](https://i.imgur.com/ubnNkgp.png)

You then learned how the streaming data workﬂow provided by Google can help  address these challenges.

You started with **Pub/Sub**, which can be used to ingest a large volume of IoT data  from diverse resources in various formats.

After that, you explored **Dataﬂow**, a serverless, NoOps service, to process the data.  ‘Process’ here refers to ETL (extract, transform, and load).

And ﬁnally, you were introduced to two Google visualization tools, **Looker** and **Data Studio**.





